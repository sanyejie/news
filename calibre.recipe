#!/usr/bin/env python
# vim:fileencoding=utf-8
from calibre.web.feeds.news import BasicNewsRecipe
import urllib
import json
import re
import time
import codecs
import datetime


class EooNews(BasicNewsRecipe):
    title = u'东财观察'
    description = u'东财观察'
    language = 'zh'
    encoding = 'UTF-8'
    timefmt = '[%F %A 　　]' #第一页日期格式。 默认：Day_Name、Day_Number Month_Name Year
    no_stylesheets = True
    remove_javascript = True
    extra_css = 'p{text-indent:1.2em;margin:0.3em 0;} \n img{display:block;text-indent:0;margin:1%;width:99%;height:auto;padding:0;border:0;} \n a{text-decoration:none;color:#000;} \n .calibre_navbar{display:none;} \n strong,h2,h1,h3,h4{font-weight:600;} \n h2,h1,h3,h4,.btc{text-indent:0;text-align:center;}'
    max_articles_per_feed = 27 #篇数，默认100篇
    max_eoo = 16
    #oldest_article = 1 #天数，默认7天，下载几天之内的文章
    recursions = 0 #文章网页上可跟踪的链接级别数
    simultaneous_downloads = 5 #同时下载数，若delay > 0，则自动减少到 1
    compress_news_images = True #压缩缩放图片
    #sort_index_by(index, weights) #根据“权重”对“索引”中的标题进行排序
    ignore_duplicate_articles = {'url'} #忽略重复文章，匹配URL或标题{'url', 'title'}
    #reverse_article_order = False #文章倒排序
    #keep_only_tags = [dict(name='div',class_='t-center')] #dict(id='topbox')] #仅保留指定的标签及其子标签
    remove_tags = [
        dict(name='div',class_=["aboutctrl","abstract","thiszihao-box-add","top-wenzhang","xd-xd-xd-rwm","xd-lj","tit01","float-right"]),
        dict(name='a',class_=["xd-a-left","right_ewm"]),
        dict(name='a',href=["https://acttg.eastmoney.com/pub/xxlout_khmbdfcfappnew_random_0007467a"]),
        dict(name='span',class_=["subtitle"]),
        dict(id=["slide_left_box"]),
        dict(name=["head","header","script","style","ul"])]
    remove_tags_before=[dict(name='div', class_=["contentwrap","contact","xd-bottom","w1200","wrap","inscon"])] #将删除第一个匹配元素前的所有标签
    remove_tags_after=[dict(name='div', class_=["txtinfos","t_box_public","xx_boxsing","xd-lj","rcon","TRS_Editor","article-page"])] #将删除第一个匹配元素后的所有标签
    preprocess_regexps = [
        (re.compile('"title">(.+?)</div>', re.S),lambda match: '"title"><h4>'+match.group(1)+'</h4></div>'),
        (re.compile(' item">(.+?)</div>.+?</a>', re.DOTALL),lambda match: 'item"><div class="btc">'+match.group(1)+'</div></div>'),
        (re.compile('<span id="space_zuozhe">(.+?)<em class="fontUp">', re.S),lambda match: '<div class="btc"><span id="space_zuozhe">'+match.group(1)+'</div><em class="fontUp">'),
        (re.compile('</h1>.+?<p>(.+?)<span>(.+?)</span>.+?a>.+?a>', re.S),lambda match: '</h1><p><div class="btc">'+match.group(1)+' <span>'+match.group(2)+'</span></div>'),
        (re.compile('<p class="k">.+?<p class="font36">(.+?)</p>\r?\n    <p class="mgb20">(.+?)</p>.+?<div class="rcon fr mgt40 por">', re.S),lambda match: '<div class="btc">'+match.group(1)+match.group(2)+'</div></div><div class="rcon fr mgt40 por">'),
        (re.compile('"pubtime_baidu">(.+?)</span>', re.S),lambda match: '"pubtime_baidu"><div class="btc">'+match.group(1)+'</div></span>'),
        (re.compile('<div style="display: none;"><img.+?</div>', re.S),lambda match: ''),
        (re.compile(' style=".+?"', re.S),lambda match: ''),
        #(re.compile('<img .{20,68}?\.png.+?>'),lambda match: ''),
        (re.compile('<div id="zoom" class="font16">.+?</div>', re.S),lambda match: '<div id="zoom" class="font16"> </div>'),
        (re.compile('">(<img src="https://jg-app(.*?)>){1}', re.S),lambda match: '"><b>该处已删</b>'),
        (re.compile('(<img src="http://upload.eeo.com.cn(.*?)>){1}', re.S),lambda match: ''),
        (re.compile('href=".+?"', re.S),lambda match: ''),
        #(re.compile('(<img src="https://np-newspic.dfcfw.com(.*?)>){1}', re.S),lambda match: ''),
        (re.compile('\u3000|<div>1</div>', re.S),lambda match: '')]

    feeds_dc = {
        '东财': 'https://finance.eastmoney.com/',
    }
    feeds_eoo1 = {
        '商业': 'https://app.eeo.com.cn/?app=article&controller=index&action=getMoreArticle&catid=3572',
    }
    feeds_eoo2 = {
        '科技': 'https://app.eeo.com.cn/?app=article&controller=index&action=getMoreArticle&catid=3549',
        '公司': 'https://app.eeo.com.cn/?app=article&controller=index&action=getMoreArticle&catid=3703',
        '研究': 'https://app.eeo.com.cn/?app=article&controller=index&action=getMoreArticle&catid=3674',
    }
    feeds_eoo3 = {
        '评论': 'https://app.eeo.com.cn/?app=article&controller=index&action=getMoreArticle&catid=3550&page=0',#&page=-1&pageSize=14
    }
    def parse_index(self):
        ans = []
        now = time.strftime("%H:%M:%S", time.localtime())
        self.eoo3(ans)
        self.dc(ans,now)
        self.eoo1(ans)
        self.eoo2(ans)
        return ans
    
    def dc(self,ans,now):
        for topic, furl in self.feeds_dc.items():
            articles = []
            news = self.get_page_content(furl).find('ul',attrs={'class':'list list_side'}).findAll('li')
            i=0
            for new in news:
                a=new.find('a')
                url=a['href']
                i+=1
                title=self.tag_to_string(a, use_alt=True).strip()
                if "北向资金" in title or "数据复盘" in title or "主力复盘" in title or "龙虎榜" in title or "十大券商策略" in title:
                    continue
                #date=time.strftime("%m-%d ",time.localtime())+re.findall(r'\d{2}:\d{2}',str(new))[0]+"" #%Y-%m-%d
                ptime=re.findall(r'\d{2}:\d{2}',str(new))[0]
                articles.append(dict(title=title, url=url, date=ptime))
            if len(articles) > 0:
                ans.append((topic, articles))
        #return ans
    
    def eoo1(self,ans):
        for topic, furl in self.feeds_eoo1.items():
            articles = []
            i=0
            try:
                news = self.fetch_data_eoo(furl)['data']
            except Exception as e:
                break
            for new in news:
                url=new['m_url']
                i+=1
                title=new['title']
                date=new['published']
                date=date[5:len(date)-3]
                articles.append(dict(title=title, url=url, date=date))
                if i>=self.max_eoo:
                    break
            if len(articles) > 0:
                ans.append((topic, articles))
        #return ans
    
    def eoo2(self,ans):
        articles = []
        ltopic = ''
        for topic, furl in self.feeds_eoo2.items():
            ltopic=topic
            i=0
            try:
                news = self.fetch_data_eoo(furl)['data']
            except Exception as e:
                break
            for new in news:
                url=new['m_url']
                i+=1
                title=new['title']
                date=new['published']
                date=date[5:len(date)-3]
                articles.append(dict(title=title, url=url, date=date))
                if i>=8:
                    break
        if len(articles) > 0:
            ans.append((ltopic, articles))
        #return ans
    
    def eoo3(self,ans):
        for topic, furl in self.feeds_eoo3.items():
            articles = []
            pages=0
            i=0
            while pages<=0:
                try:
                    news = self.fetch_data_eoo(furl)['data']
                except Exception as e:
                    break
                pages+=1
                if news is None:
                    break
                for new in news:
                    #url=new['m_url']
                    url=new['url']
                    i+=1
                    title=new['title']
                    if "台海观澜" in title or "自己的文明" in title:
                        continue
                    date=new['published']
                    date=date[5:len(date)-3]
                    articles.append(dict(title=title, url=url, date=date))
            if len(articles) > 0:
                ans.append((topic, articles))
    
    def fetch_data_json(self, path):
        return json.loads(file(path).read().decode('gbk')) #file(path)或open(path)都可以
    
    def fetch_data_eoo(self, url):
        try:
            f = urllib.request.urlopen(url).read()
            return json.loads(f)
        except Exception as e:
            self.log.error('Fetch article failed(%s): %s' % (e, url))
        return None

    def get_page_content(self, url):
        try:
            return self.index_to_soup(url)
        except Exception as e:
            self.log.error('Fetch article failed(%s): %s' % (e, url))
